<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>google cloud platform 初體驗 | 算是紀錄一下</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link href="https://fonts.googleapis.com/css?family=Noto+Serif+TC&display=swap&subset=chinese-traditional" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Ubuntu&display=swap" rel="stylesheet">
<link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" rel="stylesheet">


  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">google cloud platform 初體驗</span></h1>
<h2 class="author">deadfate</h2>
<h2 class="date">2020/07/09</h2>
</div>

<main>
<p>因為暑假參加了Tony老師的工作坊，剛好有機會試試google cloud platform這個東西。google cloud platform又簡稱GCP，跟google cloud drive不一樣，主要是提供雲端運算、資料庫架設、伺服器這些需要主機、OS、GPU這些設備的服務。</p>
<p>GCP就跟微軟的<a href="https://aws.amazon.com/tw/">Azure</a> 、<a href="https://aws.amazon.com/tw/">Amazon web services</a> 一樣，都是提供這些服務的平台，只是作為網路時代起家的google，在這部份的業務上其實落後微軟跟Amazon不少。也正因為這樣，google也很努力的宣傳他們自家的GCP。</p>
<p>那就直接進入GCP的使用吧！我是使用他們的compute engine，網路上有很多關於如何填信用卡認證、如何開一台虛擬主機（Virtual Machine）的教學，他們的介面也有使用指引，看一看應該就能成功開啟了。google很佛心的提供了300鎂的免費額度試用，可以用滿久的。</p>
<h2 id="安裝gpu-drivertensorflow跟其他需要的東西">安裝GPU driver、tensorflow跟其他需要的東西</h2>
<p>主機位置挑哪裡都可以，反正有GPU的地方就好。我的系統是選擇ubuntu 20.04，因為自己的筆電也是用這個系統（有空也來寫一篇灌系統的心得），cpu選8顆，因為要8顆才能選GPU。硬碟記得選個20G，因為10G很容易滿。GPU的話隨便挑，價格有差，但應該沒差太多，反正都是Nvidia。</p>
<p>大致節錄一下資訊</p>
<ul>
<li>CPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">xxxxxxxxxxxx@instance-1:~$ lscpu
Architecture:                    x86_64
CPU op-mode<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>:                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   <span style="color:#ae81ff">46</span> bits physical, <span style="color:#ae81ff">48</span> bits virtual
CPU<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>:                          <span style="color:#ae81ff">8</span>
On-line CPU<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> list:             0-7
Thread<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> per core:              <span style="color:#ae81ff">2</span>
Core<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span> per socket:              <span style="color:#ae81ff">4</span>
Socket<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>:                       <span style="color:#ae81ff">1</span>
NUMA node<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>:                    <span style="color:#ae81ff">1</span>
Vendor ID:                       GenuineIntel
CPU family:                      <span style="color:#ae81ff">6</span>
Model:                           <span style="color:#ae81ff">79</span>
Model name:                      Intel<span style="color:#f92672">(</span>R<span style="color:#f92672">)</span> Xeon<span style="color:#f92672">(</span>R<span style="color:#f92672">)</span> CPU @ 2.20GHz
Stepping:                        <span style="color:#ae81ff">0</span>
CPU MHz:                         2200.000
BogoMIPS:                        4400.00
Hypervisor vendor:               KVM
Virtualization type:             full
L1d cache:                       <span style="color:#ae81ff">128</span> KiB
L1i cache:                       <span style="color:#ae81ff">128</span> KiB
L2 cache:                        <span style="color:#ae81ff">1</span> MiB
L3 cache:                        <span style="color:#ae81ff">55</span> MiB
NUMA node0 CPU<span style="color:#f92672">(</span>s<span style="color:#f92672">)</span>:               0-7
</code></pre></div><ul>
<li>GPU</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">
xxxxxxxx@instance-1:~$ nvidia-smi
Thu Jul  <span style="color:#ae81ff">9</span> 10:02:27 <span style="color:#ae81ff">2020</span>       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span style="color:#f92672">===============================</span>+<span style="color:#f92672">======================</span>+<span style="color:#f92672">======================</span>|
|   <span style="color:#ae81ff">0</span>  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    <span style="color:#ae81ff">0</span> |
| N/A   34C    P0    25W / 250W |     74MiB / 16280MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|<span style="color:#f92672">=============================================================================</span>|
|    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">1011</span>      G   /usr/lib/xorg/Xorg                            61MiB |
|    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">1359</span>      G   /usr/bin/gnome-shell                          12MiB |
+-----------------------------------------------------------------------------+
</code></pre></div><ul>
<li>記憶體</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">xxxxxxxx@instance-1:~$ free -h
              total        used        free      shared  buff/cache   available
Mem:           29Gi       564Mi        27Gi       1.0Mi       1.1Gi        28Gi
Swap:            0B          0B          0B
</code></pre></div><ul>
<li>硬碟</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">xxxxxxx@instance-1:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root        19G  9.6G  8.7G  53% /
devtmpfs         15G     <span style="color:#ae81ff">0</span>   15G   0% /dev
tmpfs            15G     <span style="color:#ae81ff">0</span>   15G   0% /dev/shm
tmpfs           3.0G  1.2M  3.0G   1% /run
tmpfs           5.0M     <span style="color:#ae81ff">0</span>  5.0M   0% /run/lock
tmpfs            15G     <span style="color:#ae81ff">0</span>   15G   0% /sys/fs/cgroup
/dev/sda15      105M  3.9M  101M   4% /boot/efi
/dev/loop0       55M   55M     <span style="color:#ae81ff">0</span> 100% /snap/core18/1754
/dev/loop2       72M   72M     <span style="color:#ae81ff">0</span> 100% /snap/lxd/15896
/dev/loop1      118M  118M     <span style="color:#ae81ff">0</span> 100% /snap/google-cloud-sdk/139
/dev/loop3       30M   30M     <span style="color:#ae81ff">0</span> 100% /snap/snapd/8140
tmpfs           3.0G   20K  3.0G   1% /run/user/123
/dev/loop4       72M   72M     <span style="color:#ae81ff">0</span> 100% /snap/lxd/16044
/dev/loop5      118M  118M     <span style="color:#ae81ff">0</span> 100% /snap/google-cloud-sdk/140
tmpfs           3.0G  4.0K  3.0G   1% /run/user/1001
</code></pre></div><h3 id="安裝nivida-driver">安裝nivida driver</h3>
<p>先加入nvidia的repository</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
</code></pre></div><p>尋找可用的driver</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt-cache search nvidia-*
</code></pre></div><p>安裝</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">sudo apt install nvidia-driver-440
</code></pre></div><p>因為我是找到nvidia-driver-440，所以就裝了這版的驅動。需要注意的事，tensor的版本跟nvidia驅動、CUDA、cuDNN這3個東西的版本有互相依賴,所以記得查一下。我裝tensorflow 2.1.0，440版本的是可以執行的。</p>
<h3 id="安裝minicondatensorflow">安裝miniconda、tensorflow</h3>
<p>因為我要用conda來安裝tensorflow，所以先安裝miniconda。先去他們的<a href="https://docs.conda.io/en/latest/miniconda.html">官網</a> 最新的安裝包，然後<code>wget</code>下載。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
</code></pre></div><p>再來直接執行他</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">bash Miniconda3-latest-Linux-x86_64.sh
</code></pre></div><p>安裝好了之後，就直接使用<code>conda</code>安裝tensorflow。<a href="https://zhuanlan.zhihu.com/p/46579831">這篇文章</a> 有說明為什麼用conda裝比較好。速度快不快我不確定，不過像是 CUDA和cuDNN他也會一併幫你裝好，省下在那邊對照版本的時間。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">conda install tensorflow-gpu
conda install keras
</code></pre></div><p>裝完之後，就打開python確認一下有沒有安裝好吧！需要注意的是，因為我安裝的是tensorflow2.1.0，所以語法跟tensorflow1有點不一樣</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> tensorflow <span style="color:#f92672">as</span> tf
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>__version__)
<span style="color:#66d9ef">print</span>(tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>))
</code></pre></div><p>最後回傳<code>TRUE</code>就代表有</p>
<p>接下來就可以開始試試train model了！</p>

</main>

<p class="terms">
  
  
  Categories:
  
  <a href='/categories/statistic'>statistic</a>
  
  <a href='/categories/programming'>programming</a>
  
  
  
  
  Tags:
  
  <a href='/tags/r'>R</a>
  
  <a href='/tags/python'>python</a>
  
  
  
</p>
 <div class="article-container">
  <div id="disqus_thread"></div>
<script>





(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://deadfate.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            
</div>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  <script src="//cdn.bootcss.com/highlight.js/9.18.1/highlight.min.js"></script>
  <script src="//cdn.bootcss.com/highlight.js/9.18.1/languages/r.min.js"></script>
  <script src="//cdn.bootcss.com/highlight.js/9.18.1/languages/python.min.js"></script>
  <script src="//cdn.bootcss.com/highlight.js/9.18.1/languages/yaml.min.js"></script>
  <script>
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  </script>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156464110-1"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-156464110-1');
  </script>
  
  <hr/>
  © <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW">本站採用CC4.0授權</a> | <a href="https://github.com/deadfate-sky">Github</a>
  
  </footer>
  </body>
</html>

